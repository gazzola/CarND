{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Exploration\n",
    "\n",
    "Visualize the German Traffic Signs Dataset. This is open ended, some suggestions include: plotting traffic signs images, plotting the count of each sign, etc. Be creative!\n",
    "\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- features -> the images pixel values, (width, height, channels)\n",
    "- labels -> the label of the traffic sign\n",
    "- sizes -> the original width and height of the image, (width, height)\n",
    "- coords -> coordinates of a bounding box around the sign in the image, (x1, y1, x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# Done: fill this in based on where you saved the training and testing data\n",
    "training_file = \"dataset/train.p\"\n",
    "testing_file = \"dataset/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "print(\"Pickle files loaded succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### To start off let's do a basic data summary.\n",
    "\n",
    "# Done: number of training examples\n",
    "n_train = len(X_train)\n",
    "\n",
    "# Done: number of testing examples\n",
    "n_test = len(X_test)\n",
    "\n",
    "# Done: what's the shape of an image?\n",
    "image_shape = train['sizes'][0]\n",
    "\n",
    "# Done: how many classes are in the dataset\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Data exploration visualization goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# First, some random signs:\n",
    "\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axarr = plt.subplots(2,5, figsize=(8, 3))\n",
    "for i in range(5):\n",
    "    r1 = random.randint(0, len(X_train))\n",
    "    r2 = random.randint(0, len(X_test))\n",
    "    img_tr = X_train[r1]\n",
    "    img_ts = X_test[r2]\n",
    "    axarr[0, i].imshow(img_tr)\n",
    "    axarr[0, i].axis(\"off\")\n",
    "    axarr[0, i].set_title(\"X_train[%d]\" % r1, fontsize=8)\n",
    "    axarr[1, i].imshow(img_ts)\n",
    "    axarr[1, i].axis(\"off\")\n",
    "    axarr[1, i].set_title(\"X_test[%d]\" % r2, fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Then, one of each class, to know what I'm dealing with.\n",
    "\n",
    "y_list = list(y_test)\n",
    "total = len(y_list)\n",
    "fig, axarr = plt.subplots(9,5, figsize=(8, 15))\n",
    "i, j = 0, 0\n",
    "for label in set(y_list):\n",
    "    img = X_test[y_list.index(label)]\n",
    "    freq = 100. * y_list.count(label) / total\n",
    "    axarr[i, j].imshow(img)\n",
    "    axarr[i, j].axis(\"off\")\n",
    "    axarr[i, j].set_title(\"Class #%d (%.2f%%)\" % (label, freq), fontsize=8)\n",
    "    j += 1\n",
    "    if j > 4:\n",
    "        j = 0\n",
    "        i += 1\n",
    "axarr[8, 3].axis(\"off\")\n",
    "axarr[8, 4].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Your model can be derived from a deep feedforward net or a deep convolutional network.\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README: Proposal\n",
    "\n",
    "I'll be trying to implement [Ciresan _et al._'s 2012 algorithm](http://people.idsia.ch/~juergen/nn2012traffic.pdf). They implemented a multi-column DNN, or \"DNN committee\", to achieve the [top performance in the GTSRB competition](http://benchmark.ini.rub.de/?section=gtsrb&subsection=results), with 99.46% precision. Figure 1 of the linked paper describes the approach, which in a very simplified manner is:\n",
    "\n",
    "1. Each input is preprocessed by different methods;\n",
    "2. Each preprocessed input is fed to a number of DNNs;\n",
    "3. Output of all DNNs is averaged for each image.\n",
    "\n",
    "Ciresan'12 reports that each individual DNN in the ensemble achieved precision above 98%. My personal target for this submission is 95% precision, although I'd be happy with 90% and willing to accept anything above 80% at this stage, due to time constraints related to the course timeline. Due to my humbler aspirations, a number of simplifications will be made on the original algorithm:\n",
    "\n",
    "1. A single DNN will be used instead of an ensemble, to avoid the computational cost associated with training multiple neural nets on thousands of images;\n",
    "2. Because of the above, a single preprocessing method will be used instead of three;\n",
    "3. During training, the distortion step will be dropped.\n",
    "\n",
    "These changes are documented and explained more thoroughly in the corresponding sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Preprocess the data here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def preprocess(image):\n",
    "    # presets\n",
    "    img_size = 48 # image height/width\n",
    "    border = img_size // 10 # 10% borders\n",
    "    new_size = img_size + (2 * border)\n",
    "    new_shape = (new_size, new_size) # shape of the target image\n",
    "    \n",
    "    # 1. Resample image\n",
    "    img_rs = np.zeros(new_shape)\n",
    "    # a. If image size is new_size, do nothing:\n",
    "    if image.shape == new_shape:\n",
    "        img_rs = image\n",
    "    # b. If image is larger, shrink using recommended downsizing interpolation\n",
    "    elif image.shape > new_shape:\n",
    "        img_rs = cv2.resize(src = image, dsize = new_shape, interpolation = INTER_AREA)\n",
    "    # c. If image is smaller, zoom using bilinear interpolation, which is the default\n",
    "    else: \n",
    "        img_rs = cv2.resize(src = image, dsize = new_shape)\n",
    "    \n",
    "    # 2. Crop the borders of the image\n",
    "    img_48 = img_rs[border:-border,border:-border]\n",
    "    \n",
    "    # 3. Normalize image as per sermanet'11\n",
    "    # a. Convert image to YUV space\n",
    "    yuv = cv2.cvtColor(img_48, cv2.COLOR_BGR2YUV)\n",
    "    # b. Normalize Y channel using adaptive histogram equalization, chosen from Ciresan'12\n",
    "    clahe = cv2.createCLAHE(clipLimit = 5, tileGridSize=(6,6))\n",
    "    yuv[:,:,0] = clahe.apply(yuv[:,:,0])\n",
    "    norm = cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "    return norm\n",
    "\n",
    "def rescale(matrix):\n",
    "    # Maps an RGB image matrix from 0:255 to -1:1\n",
    "    nmatrix = np.zeros_like(matrix, dtype=np.float32)\n",
    "    if len(matrix.shape) == 2:\n",
    "    # Greyscale:\n",
    "        return (2. * matrix / 255.) - 1.\n",
    "    # Colour:\n",
    "    for i in range(matrix.shape[-1]):\n",
    "        nmatrix[:,:,i] = (2. * matrix[:,:,i] / 255.) - 1.\n",
    "    return nmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test 1: particularly bad images chosen at random by visual inspection\n",
    "bad_train = [X_train[16287], X_train[24649], X_train[18641], X_train[30115], X_train[19010]]\n",
    "bad_test = [X_test[1618], X_test[1339], X_test[9265], X_test[11413],X_test[3301]]\n",
    "\n",
    "fig, axarr = plt.subplots(4,5, figsize=(8, 6))\n",
    "for i in range(5):\n",
    "    img_tr = bad_train[i]\n",
    "    fix_tr = preprocess(img_tr)\n",
    "    img_ts = bad_test[i]\n",
    "    fix_ts = preprocess(img_ts)\n",
    "    axarr[0, i].imshow(img_tr)\n",
    "    axarr[0, i].axis(\"off\")\n",
    "    axarr[0, i].set_title(\"bad_train[%d]\" % i, fontsize=8)\n",
    "    axarr[1, i].imshow(fix_tr, cmap=\"Greys_r\")\n",
    "    axarr[1, i].axis(\"off\")\n",
    "    axarr[1, i].set_title(\"fix_train[%d]\" % i, fontsize=8)\n",
    "    axarr[2, i].imshow(img_ts)\n",
    "    axarr[2, i].axis(\"off\")\n",
    "    axarr[2, i].set_title(\"bad_test[%d]\" % i, fontsize=8)\n",
    "    axarr[3, i].imshow(fix_ts, cmap=\"Greys_r\")\n",
    "    axarr[3, i].axis(\"off\")\n",
    "    axarr[3, i].set_title(\"fix_test[%d]\" % i, fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test 2: random images\n",
    "\n",
    "fig, axarr = plt.subplots(4,5, figsize=(8, 6))\n",
    "for i in range(5):\n",
    "    r1 = random.randint(0, len(X_train))\n",
    "    r2 = random.randint(0, len(X_test))\n",
    "    img_tr = X_train[r1]\n",
    "    fix_tr = preprocess(img_tr)\n",
    "    img_ts = X_test[r2]\n",
    "    fix_ts = preprocess(img_ts)\n",
    "    axarr[0, i].imshow(img_tr)\n",
    "    axarr[0, i].axis(\"off\")\n",
    "    axarr[0, i].set_title(\"X_train[%d]\" % r1, fontsize=8)\n",
    "    axarr[1, i].imshow(fix_tr, cmap=\"Greys_r\")\n",
    "    axarr[1, i].axis(\"off\")\n",
    "    axarr[1, i].set_title(\"fix_train[%d]\" % r1, fontsize=8)\n",
    "    axarr[2, i].imshow(img_ts)\n",
    "    axarr[2, i].axis(\"off\")\n",
    "    axarr[2, i].set_title(\"X_test[%d]\" % r2, fontsize=8)\n",
    "    axarr[3, i].imshow(fix_ts, cmap=\"Greys_r\")\n",
    "    axarr[3, i].axis(\"off\")\n",
    "    axarr[3, i].set_title(\"fix_test[%d]\" % r2, fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test 3: one from each class\n",
    "\n",
    "y_list = list(y_train)\n",
    "total = len(y_list)\n",
    "fig, axarr = plt.subplots(9,5, figsize=(8, 15))\n",
    "i, j = 0, 0\n",
    "for label in set(y_list):\n",
    "    img = preprocess(X_train[y_list.index(label)])\n",
    "    freq = 100. * y_list.count(label) / total\n",
    "    axarr[i, j].imshow(img)\n",
    "    axarr[i, j].axis(\"off\")\n",
    "    axarr[i, j].set_title(\"Class #%d (%.2f%%)\" % (label, freq), fontsize=8)\n",
    "    j += 1\n",
    "    if j > 4:\n",
    "        j = 0\n",
    "        i += 1\n",
    "axarr[8, 3].axis(\"off\")\n",
    "axarr[8, 4].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X_train_prep = np.array([rescale(preprocess(img)) for img in X_train])\n",
    "X_test_prep = np.array([rescale(preprocess(img)) for img in X_test])\n",
    "\n",
    "### Repeating the basic data summary for the preprocessed data.\n",
    "\n",
    "n_train = X_train_prep.shape\n",
    "n_test = X_test_prep.shape\n",
    "image_shape = X_train_prep[0].shape\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "_Describe the techniques used to preprocess the data._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Steps within the `preprocess` function:\n",
    "Steps within the preprocess function:\n",
    "1. Resample image to 56x56;\n",
    "2. Crop 8 pixels from the borders to yield a 48 x 48 image;\n",
    "3. Normalise using Contrast Limited Adaptive Histogram Equalisation, or [CLAHE](https://en.wikipedia.org/wiki/Adaptive_histogram_equalization), on the Y channel of YUV image to improve contrast. Parameters: clip limit of 1, 6x6 grid.\n",
    "\n",
    "Algorithm chosen was Adaptive Histogram Equalization (AHE), because this method produced the most stable results in Ciresan'12. I'm not sure if the AHE used in that work was contrast limited. However, results obtained are, upon visual inspection of small samples, similar to the ones reported. \n",
    "\n",
    "I'm also not sure if the border trimming reported in Ciresan'12 was performed computationally, but this is the only viable method for this work. It'd be possible to attempt this method without the cropping step. The reference paper describes trimming borders of \"around 10%\".\n",
    "\n",
    "The preprocessing step seemingly discarded mostly useless information, and greatly improve the visibility of very dark images, even allowing for color recognition (see test 1, case `bad_train[3]`). Quality of images with better visibility isn't terribly affected, although blue signs with decent visibility seem to suffer a bit (see test 3, classes 14 and 34 to 42).\n",
    "\n",
    "An additional function, `rescale`, maps the image from `[0,255]` to `[-1.0,1.0]`, to improve the performance of normalization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Additional Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Generate data additional (if you want to!)\n",
    "### and split the data into training/validation/testing sets here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "_Describe how you set up the training, validation and testing data for your model. If you generated additional data, why?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Ciresan'12 used distortions in the dataset, introduced before the start of each epoch of training, to generate randomised extra samples. This method won't be implemented in this work unless results obtained without it are below expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Define your architecture here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import tensorflow as tf\n",
    "\n",
    "### Parameters\n",
    "img_size = 48\n",
    "\n",
    "# convolutions\n",
    "stc   = [1, 1, 1, 1]     # stride used in convolution steps\n",
    "conv1 = (7, 7, 3, 100)   # shape of the first convolution filter\n",
    "conv2 = (4, 4, 100, 150) # shape of the second convolution filter\n",
    "conv3 = (4, 4, 150, 250) # shape of the third convolution filter\n",
    "\n",
    "# max pooling\n",
    "stp  = [1, 2, 2, 1] # stride used in max pooling steps\n",
    "pwnd = (1, 2, 2, 1) # shape of the max pooling windows\n",
    "\n",
    "# fully connected layers\n",
    "mult1 = (2250, 300) # fully connected layer weights shape\n",
    "mult2 = (300, 43)   # output layer weights shape\n",
    "\n",
    "\n",
    "### Placeholders\n",
    "inputs = tf.placeholder(tf.float32, shape = (None, img_size, img_size, 3), name = 'inputs')\n",
    "labels = tf.placeholder(tf.float32, shape = (None, n_classes), name = 'labels')\n",
    "\n",
    "### Filters/Weights:\n",
    "weights = {\n",
    "    \"conv1\": tf.Variable(tf.random_normal(shape = conv1)), # convolution filter\n",
    "    \"conv2\": tf.Variable(tf.random_normal(shape = conv2)), # convolution filter\n",
    "    \"conv3\": tf.Variable(tf.random_normal(shape = conv3)), # convolution filter\n",
    "    \"mult1\": tf.Variable(tf.random_normal(shape = mult1)), # fully connected layer weights\n",
    "    \"mult2\": tf.Variable(tf.random_normal(shape = mult2))  # output layer weights\n",
    "          }\n",
    "### Biases\n",
    "biases = {\n",
    "    \"conv1\": tf.Variable(tf.random_normal(shape = conv1[-1:])),\n",
    "    \"conv2\": tf.Variable(tf.random_normal(shape = conv2[-1:])),\n",
    "    \"conv3\": tf.Variable(tf.random_normal(shape = conv3[-1:])),\n",
    "    \"mult1\": tf.Variable(tf.random_normal(shape = mult1[-1:])),\n",
    "    \"mult2\": tf.Variable(tf.random_normal(shape = mult2[-1:]))\n",
    "         }\n",
    "\n",
    "### Operations:\n",
    "# first convolution\n",
    "layer1 = tf.nn.conv2d(input = inputs, filter = weights[\"conv1\"],\n",
    "                      strides = stc, padding = \"VALID\")\n",
    "layer1 = tf.nn.bias_add(layer1, biases[\"conv1\"])\n",
    "\n",
    "# first max pooling\n",
    "layer2 = tf.nn.max_pool(value = layer1, ksize = pwnd,\n",
    "                        strides = stp, padding = \"VALID\")\n",
    "layer2 = tf.tanh(layer2)\n",
    "\n",
    "# second convolution\n",
    "layer3 = tf.nn.conv2d(input = layer2, filter = weights[\"conv2\"],\n",
    "                      strides = stc, padding = \"VALID\")\n",
    "layer3 = tf.nn.bias_add(layer3, biases[\"conv2\"])\n",
    "\n",
    "# second max pooling\n",
    "layer4 = tf.nn.max_pool(value = layer3, ksize = pwnd,\n",
    "                        strides = stp, padding = \"VALID\")\n",
    "layer4 = tf.tanh(layer4)\n",
    "\n",
    "# third convolution\n",
    "layer5 = tf.nn.conv2d(input = layer4, filter = weights[\"conv3\"],\n",
    "                      strides = stc, padding = \"VALID\")\n",
    "layer5 = tf.nn.bias_add(layer5, biases[\"conv3\"])\n",
    "\n",
    "# third max pooling\n",
    "layer6 = tf.nn.max_pool(value = layer5, ksize = pwnd,\n",
    "                        strides = stp, padding = \"VALID\")\n",
    "layer5 = tf.tanh(layer5)\n",
    "\n",
    "# fully connected layer\n",
    "layer7 = tf.matmul(tf.reshape(layer6, [-1, 2250]), weights[\"mult1\"])\n",
    "layer7 = tf.tanh(tf.nn.bias_add(layer7, biases[\"mult1\"]))\n",
    "\n",
    "# output layer\n",
    "output = tf.matmul(layer7, weights[\"mult2\"])\n",
    "output = tf.nn.softmax(tf.nn.bias_add(output, biases[\"mult2\"]))\n",
    "\n",
    "print(\"Architecture built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)  For reference on how to build a deep neural network using TensorFlow, see [Deep Neural Network in TensorFlow\n",
    "](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/b516a270-8600-4f93-a0a3-20dfeabe5da6/concepts/83a3a2a2-a9bd-4b7b-95b0-eb924ab14432) from the classroom._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Same architecture will be used here as in Ciresan'12. These are the layers used:\n",
    "\n",
    "0. Input → 48×48 neurons with 3 maps (RGB image regularised to [-1,1])\n",
    "1. Convolution (7×7 filter) → 42×42 neurons with 100 maps\n",
    "2. Max-pooling (2×2 window) → 21×21 neurons with 100 maps\n",
    "3. Convolution (4×4 filter) → 18×18 neurons with 150 maps\n",
    "4. Max-pooling (2×2 window) → 9×9 neurons with 150 maps\n",
    "5. Convolution (4×4 filter) → 6×6 neurons with 250 maps\n",
    "6. Max-pooling (2×2 window) → 3×3 neurons with 250 maps\n",
    "7. Fully connected → 300 neurons\n",
    "8. Output → 43 neurons\n",
    "\n",
    "Activation function is a hyperbolic function in the fully connected and max-pooling layers. The output layer uses a softmax function. A bias is added to each convolution and fully connected layer. All convolution layers use a stride  of one, while max-pooling layers use a stride of two. Valid padding is used whenever applicable.\n",
    "\n",
    "Training time reported in the paper was 37h for an ensemble of 25 DNNS, using a machine with 3.3GHz, 24GB DDR3 and four GTX580 GPUs. I'm using one with 2.7GHz, 16GB DDR3 and one GT650 GPU. It's hard to accurately estimate, but I'm hoping a single DNN with this configuration will be successfully trained overnight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Train your model here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "\n",
    "### TrainingSetup\n",
    "\n",
    "# Parameters:\n",
    "learning_rate = 0.5\n",
    "training_epochs = 1\n",
    "batch_size = 500\n",
    "\n",
    "# Optimization\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output, labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "print(\"Setup done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Training\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    # Training cycle\n",
    "    for _ in range(training_epochs):\n",
    "        for i in range(0, n_train, batch_size):\n",
    "            batch_x = X_train_prep[i : i + batch_size]\n",
    "            batch_y = y_train[i : i + batch_size]\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            # session.run(optimizer, feed_dict={inputs: batch_x, labels: batch_y})\n",
    "            print(batch_x.shape)\n",
    "            print(batch_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "_How did you train your model? (Type of optimizer, batch size, epochs, hyperparameters, etc.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "\n",
    "_What approach did you take in coming up with a solution to this problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I didn't have much prior experience with DNNs, and if I were to implement from scratch, I'd either \"come up\" with what I had seen in class (_i.e._ use the same everything) or waste time pursuing counterproductive approaches.\n",
    "\n",
    "Therefore, I did what I know how to: I researched the literature on this exact problem, took a brief look at [a](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf) [few](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DeepConvexNetwork-Interspeech2011-pub.pdf) [approaches](http://yann.lecun.com/exdb/publis/pdf/ranzato-06.pdf)\\* and chose [Ciresan _et al._, 2012](http://people.idsia.ch/~juergen/nn2012traffic.pdf), because:\n",
    "\n",
    "1. It was designed for this specific purpose, and had top performance in the competition;\n",
    "2. At a brief look, the method seemed wellvery well explained in the paper, so reimplementing it should be straightforward even for someone with little `tensorflow` experience;\n",
    "3. The algorithm described showed several opportunities for simplification;\n",
    "4. There was sufficient reason to believe that even the simplified version of the algorithm would perform decently, due to a number of tests also reported in the paper.\n",
    "\n",
    "<sub>\\* Two things worth noting here. First: I didn't read all of those texts in their entirety, a brief look at the layout and equations presented allows to tell which ones would be easier to implement, namely the one I used and the one cited in the instructions of this notebook. \n",
    "\n",
    "<sub>Second: some of those are on digit recognition, but it's a similar enough task. If I were to implement any of them, I'd just have added to the preprocessing task a pre-classifier based on colour and/or shape (to divide the 43 classes into a smaller amount) and a binariser, to try and extract the symbol in the centre. I'm not sure how hard this would've been to achieve, and it seems like an interesting endeavour.\n",
    "\n",
    "<sub>However, given that I'm taking this course to learn new things, I decided to go for the deep/conv net approach. The alternative method is just an image manipulation task followed by a classifier similar to the one used in the not-MNIST lab, and I already know those things better than I know DNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "Take several pictures of traffic signs that you find on the web or around you (at least five), and run them through your classifier on your computer to produce example results. The classifier might not recognize some local signs but it could prove interesting nonetheless.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "_Choose five candidate images of traffic signs and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult? It would be helpful to plot the images in the notebook._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run the predictions here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "_Is your model able to perform equally well on captured pictures or a live camera stream when compared to testing on the dataset?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize the softmax probabilities here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "*Use the model's softmax probabilities to visualize the **certainty** of its predictions, [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#top_k) could prove helpful here. Which predictions is the model certain of? Uncertain? If the model was incorrect in its initial prediction, does the correct prediction appear in the top k? (k should be 5 at most)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "_If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired images._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd]",
   "language": "python",
   "name": "conda-env-carnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
